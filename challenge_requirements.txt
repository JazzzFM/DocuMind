ML Coding Challenge: Document Classification and Entity Extraction System

Overview
In this challenge, you will build a system that processes various document types (invoices,
assignments, forms, etc.) using OCR, vector databases, and AI models to identify document types and
extract relevant entities. This challenge simulates real-world document processing scenarios across
different business domains.

[! IMPORTANT] Dataset is now in the path: docs-sm/

You'll be working with a dataset containing multiple document types. The dataset includes various
business documents such as invoices, assignments, forms, and other document types.
Dataset Link: Document Dataset

Requirements

1. Document Processing Pipeline
Create a document processing pipeline that:
Extracts text from documents using an OCR service of your choice
Processes and cleans the extracted text
Identifies document type (invoice, assignment, form, etc.)
Extracts relevant entities based on document type
Stores documents, document types, and extracted entities in a vector database (ChromaDB)

2. Django Management Command
Implement a Django management command that:
Takes the dataset as input
Processes each document through your pipeline
Identifies the document type for each document
Upserts documents and their type into ChromaDB
Handles errors gracefully and provides logging

3. Django API View
Design and implement a Django API view that:
Accepts a document file upload from a user
Extracts text using your OCR service
Queries the vector database to identify document type
Calls an LLM model of your choice to extract entities from the document
Returns a structured JSON response with document type and extracted entities

4. Documentation
Provide comprehensive documentation including:
A detailed README.md with step-by-step instructions for setting up and running your solution
Architecture overview explaining your design choices
API documentation
Instructions for adding new document types or expanding functionality
Evaluation Criteria
Your solution will be evaluated based on:
. Functionality: Does it accurately identify document types and extract relevant entities?
. Code Quality: Is the code well-structured, readable, and maintainable?
. Performance: How efficiently does it process documents and handle queries?
. Documentation: Is the documentation clear and comprehensive?
. Innovation: Creative approaches to solving the challenges

Technical Guidelines
Use Python and Django for the implementation. Use ChromaDB as your vector database for document storage and retrieval. You may use any OCR service of your choice (e.g., Tesseract, Google Vision, Azure Form
Recognizer) You may use any LLM model of your choice (e.g., OpenAI, Hugging Face models, etc.) for entity
extraction. Provide requirements.txt or environment.yml for dependencies

Include unit tests for critical components
Submission Instructions

Submit your solution as a Git repository with:
. All source code
. README.md with step-by-step setup and usage instructions
. Any configuration files needed
. Sample outputs demonstrating functionality (do not include the full dataset)

Example Implementation Steps
Here's a high-level overview of steps you might follow:
. Set up Django project structure with appropriate apps and modules
. Implement OCR functionality to extract text from documents
. Create document classification logic to identify document types
. Design entity extraction for different document types
. Set up ChromaDB integration for vector storage
. Implement the Django management command for batch processing
. Create the Django API view for single document processing
. Integrate with an LLM model for document type identification and entity extraction
. Write comprehensive tests and documentation


I wanted to share with you the KPIs and evaluation criteria that the client typically uses when reviewing submissions. We've observed that aligning with these metrics significantly increases the chances of success in the challenge, so weâ€™re passing them along to help you focus your efforts on what truly matters to them.


KPIS: 
These KPIs cover functionality, architecture, performance, and usability, and they reflect the standards and priorities we've seen from the client in similar technical evaluations. By keeping these in mind during your implementation, youâ€™ll be able to target your solution more effectively.

Please feel free to reach out if you have any questions or need further clarification â€” weâ€™re here to support you throughout the process.


âœ… Functional Quality
Document classification accuracy: Should correctly identify document types (e.g., invoice, form) with â‰¥ 90% accuracy.
Entity extraction precision: Target â‰¥ 85% accuracy in extracting relevant fields using LLMs.
OCR robustness: Ensure extracted text is accurate and clean even from rotated or noisy scans.
Multi-format support: System should handle PDF, PNG, JPG at a minimum.

âœ… Technical Architecture & Code Quality
Modular structure: Clear separation between OCR, classification, vectorization, LLM interaction, and API logic.
Testing coverage: Include unit tests for critical modules (aim for 85%+ coverage).
Error handling: Graceful handling of broken files, timeouts, LLM failures, and user errors.
Code readability: Clean, well-structured code with proper naming, comments, and adherence to best practices.

âœ… Performance & Scalability
Processing speed: Complete OCR + classification + LLM extraction in â‰¤ 3 seconds per document.
API stability: Handle at least 5 concurrent requests without performance degradation.
Resource efficiency: System should not consume excessive memory or CPU on reasonable workloads.

âœ… Integration & Implementation
ChromaDB integration: Store vectorized documents with metadata, and support queries.
LLM prompting: Design clear, structured prompts and manage temperature, retries, and failures effectively.
OCR engine flexibility: Use a wrapper that supports easy engine replacement (e.g., Tesseract, Google Vision).

âœ… Documentation & Usability
README completeness: Should include setup, usage, dependencies, environment variables, and examples.
Deployment: Project should run with docker-compose, Makefile, or equivalent to simplify review.
Extensibility: Code should allow for adding new document types or fields with minimal changes.

ðŸ’¡ Bonus Points (Not required, but valued)
Embedding fine-tuning, caching for LLM calls, or multi-step prompting.
Input validation for malformed or malicious files.
Insightful architectural trade-offs explained in the documentation.
